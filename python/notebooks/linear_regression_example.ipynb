{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aee506d8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Basic Regression\n",
    "\n",
    "For this, we'll do a simple linear regression as described [on this Wikipedia page](https://en.wikipedia.org/wiki/Simple_linear_regression), creating a line of best fit through a series of points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f777df",
   "metadata": {},
   "source": [
    "## Creating your virtual environment\n",
    "\n",
    "We'll install packages locally to the folder you're doing these exercises. This avoids conflicts with PC priveleges and To do this, let's create a virtual environment. The following commands will be run assuming you are using a Windows machine.\n",
    "\n",
    "Run the following from a terminal:\n",
    "\n",
    "```\n",
    "py -m venv lab\n",
    "```\n",
    "\n",
    "A virtual environment should be created in your directory. Look for a folder named `lab`. See what's inside. Look in the windows explorer, or type the following to see the contents:\n",
    "\n",
    "```\n",
    "tree lab\n",
    "```\n",
    "\n",
    "Inside a directory called `Scripts`, there is an `activate` file. In terminal, type the file: `lab\\Scripts\\activate`. Now, your virtual environment is activated, as indicated with the text `(lab)` at the front of the terminal entry line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a58cae-f37f-4840-b265-41bb1bbb841e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Manually Calculating the Linear Regression\n",
    "\n",
    "To get familiar with some python basics, we will manually perform a linear regression in R. Following sections will take advantage of existing libraries to do the leg-work for you.\n",
    "\n",
    "For this, we'll use `numpy`. Numpy is one of the most common python scientific packages. It includes a multi-dimensional array and some high-\n",
    "performance functions and operations on those arrays. I'd highly recommend viewing [their documentation](https://numpy.org/doc/stable/) to get a more-complete understanding. At the very least, go through [their absolute beginners tutorial](https://numpy.org/doc/stable/user/absolute_beginners.html) if you're starting from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c522af01-5fae-43ba-8589-5b7584bcc5e0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Reading the data from file\n",
    "\n",
    "We start by importing it. Because it is such a common library, it is convention to shorten the name of `numpy` to `np` with the `as` keyword, as seen below. We'll load the data from file using `np.genfromtxt`.\n",
    "\n",
    "Before using the function, briefly review its documentation (either with the `help()` function or with `pydoc3`).\n",
    "\n",
    "* We'll use the `delimiter=','` option to say that entries are comma-separated.\n",
    "* We'll use the `skip_header=True` option to state that we don't care about the names of the columns.\n",
    "* We'll use the `dtype=np.int64` to state that the incoming variables are integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039884a4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # for performing arithmetic on homogeneous arrays\n",
    "\n",
    "xy = np.genfromtxt('linear_regression_data.csv', delimiter=',', skip_header=True, dtype=np.int64)\n",
    "print(f\"  x y\\n{xy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ff2b8f-cd62-4afc-92ea-02711d1ddc52",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Finding the mean and deviations from mean\n",
    "\n",
    "We can now take this array and use it going forward to find the mean on each column, using the same `numpy` library. Afterwards, we'll find the deviation of each x and y value from the mean.\n",
    "\n",
    "We have to take the mean of each column of our 2D xy array. To do this, we use the `axis` argument. This is often quite confusing to interpret, even for intermediate users, but we specify as the `axis` which dimension we wish to aggregate or collapse down. `axis=0` means to collapse the 0th-indexed dimension, or row. `axis=1` means to collapse the columns. In other words, for a `mean`, specifying `axis=0` will take all row entries per column and find the mean of them. This is what we want.\n",
    "\n",
    "If you ever feel confused, don't be afraid to experiment. Take a small data sample, and run the mean for `axis=0`, then `axis=1` and see which looks right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806fdee9-e61c-4946-b7bd-ba8462b115f3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "xy_mean = xy.mean(axis=0) # axisaxis 0 is rows, axis 1 is columns. Take the mean of \n",
    "\n",
    "print(f\"Mean of x {xy_mean[0]} and mean of y {xy_mean[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600c5ca2",
   "metadata": {},
   "source": [
    "Basic arithmetic of numpy arrays will be performed element-by-element, given that they are the same dimension. When not the same dimension, `numpy` will try to [broadcast](https://numpy.org/doc/stable/user/basics.broadcasting.html) the dimensions to perform element-wise arithmetic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2162c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_dev = xy - xy_mean\n",
    "print(f\"Deviations (or least squares residuals) of x and y from its mean:\\n  xdev ydev\\n{xy_dev}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d727bc1c",
   "metadata": {},
   "source": [
    "### Slope\n",
    "\n",
    "We can now calculate the slope using the formula:\n",
    "\n",
    "$$m = \\frac{\\sum ((x_i - \\bar{x}) * (y_i - \\bar{y}))}{\\sum ((x_i - \\bar{x})^2)}$$\n",
    "\n",
    "or:\n",
    "\n",
    "$$m = \\frac{\\sum ( \\textrm{x\\_dev} * \\textrm{y\\_dev})}{\\sum ( \\textrm{x\\_dev}^2)}$$\n",
    "\n",
    "We can use `numpy.sum` to calculate the sum of each point, and the division operator to divide the two sums `/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4245433e-aaec-42ed-9cfe-59c072651be1",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "regression_slope = np.sum(xy_dev[:,0] * xy_dev[:,1]) / np.sum(xy_dev[:,0]**2)\n",
    "print(f\"slope = {regression_slope}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877f13da",
   "metadata": {},
   "source": [
    "Now that we have the slope and mean, the intercept can be found using the formula\n",
    "\n",
    "$$c = \\bar{y} - m * \\bar{x}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8f63a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_intercept = xy_mean[1] - regression_slope * xy_mean[0]\n",
    "print(f\"intercept = {regression_intercept}\")\n",
    "\n",
    "print(f\"In the form y = a + bx, we have:\\n  y = {regression_intercept:.2f} + {regression_slope:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1c7747",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Plotting\n",
    "\n",
    "We'll create a function to plot the points and best fit line, so that we can call it throughout this notebook. To do this, we'll use `matplotlib`: see [documentation here](https://matplotlib.org/stable/api/index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea8747b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_best_fit(x, y, slope, intercept):\n",
    "    plt.plot(x,y, 'o', label=\"Example data\", markersize=10)\n",
    "    plt.plot(x, slope * x + intercept, label=\"Fitted line\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "plot_best_fit(x,y, regression_slope, regression_intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f36f92e",
   "metadata": {},
   "source": [
    "### Finding Sum of Squared values and R-squared\n",
    "\n",
    "We will calculate the following 3 sum of square values:\n",
    "\n",
    "1. Sum of Squares Total -- SST\n",
    "2. Sum of Squares Regression -- SSR\n",
    "3. Sum of Squares Estimate of Errors -- SSE\n",
    "\n",
    "The equations are as follows\n",
    "\n",
    "$$\n",
    "SST = \\sum (y_i - \\bar{y}) \\\\\n",
    "SSR = \\sum (\\hat{y}_i â€“ \\bar{y})^2 \\\\\n",
    "SSE = \\sum (y_i - \\hat{y_i})^2\n",
    "$$\n",
    "\n",
    "Where $y_i$ is each observed y entry, $\\hat{y}_i$ is the predicted y entry and $\\bar{y}$ is the mean.\n",
    "\n",
    "Note the following relationship:\n",
    "$$SST = SSR + SSE$$\n",
    "\n",
    "We can calculate all three with our previously calculated values in `xy_dev` and `xy_mean`, and `np.sum`. Just as we did in the plot above, we can calculate all the predicted values with our initial `x` and store it in a variable called `y_hat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056b1a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sst = np.sum(xy_dev[:,1]**2)\n",
    "print(f\"Sum of Squares Total: {sst:.4f}\")\n",
    "\n",
    "y_hat = x * regression_slope + regression_intercept\n",
    "ssr = np.sum((y_hat - xy_mean[1])**2)\n",
    "print(f\"Sum of Squares Regression: {ssr:.4f}\")\n",
    "\n",
    "sse = sst - ssr\n",
    "print(f\"Sum of Squares Estimate of Errors: {sse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3313078",
   "metadata": {},
   "source": [
    "Finally, we can calculate R-squared with the following formula:\n",
    "$$R^2 = \\frac{SSR}{SST}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9955048",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_squared = ssr / sst\n",
    "print(f\"r-squared = {r_squared}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50edb62-cbcf-4fdd-9a3e-5e7f1233e1a2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## The SciPy Solution\n",
    "\n",
    "Performing a linear regression in python is easy with the [scipy](https://docs.scipy.org/doc/scipy/index.html) library. Below shows an example of generated output with arbitrary inputs. We'll use the [linregress](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.linregress.html) function found in the stats module of scipy. It will return a variety of variables, including slope, intercept and many others.\n",
    "\n",
    "We'll also use [numpy](https://numpy.org/doc/stable/) throughout this tutorial. It is contains a useful and efficient way to store and manipulate arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaab9e59",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "lin_reg = scipy.stats.linregress(x,y)\n",
    "\n",
    "print(f\"Printing object shows all returned data:\\n{lin_reg}\\n\")\n",
    "\n",
    "print(\"Let's break down what was returned.\\n\"\n",
    "      f\"  * slope ({lin_reg.slope:.2f}) and intercept ({lin_reg.intercept:.2f}) are used to find line of best fit.\\n\"\n",
    "      f\"  * r-value ({lin_reg.rvalue:.2f}), or Pearson's coefficient, which we can use to get to r-squared,\\n\"\n",
    "      f\"    the coeffecient of determination ({lin_reg.rvalue**2:.4f})\\n\"\n",
    "      f\"  * p-value for hypothesis testing ({lin_reg.pvalue:.2f}).\\n\"\n",
    "      f\"    The alternative hypothesis can be set in function call with 'alternative' argument.\\n\"\n",
    "      f\"  * standard error of the slope ({lin_reg.stderr:.2f}) and of the intercept ({lin_reg.intercept_stderr:.2f}), assuming residual normality\"\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8363361c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "We can use the function we created above to plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fce47dc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_best_fit(x, y, lin_reg.slope, lin_reg.intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0444862-7f30-4fd6-b1d1-0e876c3af8e7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Using numpy Least Squares function\n",
    "\n",
    "If we wanted to solve a least-squares regression more optimally using only `numpy`, we can use linear algebra and a tool available in `numpy` already. The `numpy` library has a linear algebra submodule and in there is a least-square solution: `numpy.linalg.lstsq`, see [the help page](https://numpy.org/doc/stable/reference/generated/numpy.linalg.lstsq.html) for their own example.\n",
    "\n",
    "The result of `np.linalg.lstsq` provides a few useful parameters for the data given, such as the solution itself (slope and intercept) along with average of the squared residuals, matrix rank, etc. Really, we're interested in the first things it returns, the solution (i.e. slope, intercept)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cda710a-653b-4383-9587-f44d4a6a60d5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "A = np.array([x, np.ones(len(x))]).T # T is shortcut for \"transpose()\"\n",
    "lstsq_out = np.linalg.lstsq(A, y, rcond=None) # Finding the least-squares solution to Ax = y\n",
    "\n",
    "slope, intercept = lstsq_out[0]\n",
    "print(f\"slope = {slope}, intercept = {intercept}\")\n",
    "print(f\"sum of residuals, squared (or SSE): {lstsq_out[1]}\")\n",
    "print(f\"matrix rank = {lstsq_out[2]}\")\n",
    "print(f\"singular values of input (A): {lstsq_out[3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f2c331-4449-4a83-b804-15abd05cb707",
   "metadata": {},
   "source": [
    "Again, we can plot our results, as we saw before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32938e1-92b2-4496-bdc3-f39c98cf8914",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_best_fit(x,y, slope, intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76da7224",
   "metadata": {},
   "source": [
    "## Using statsmodel to tell us everything in the universe\n",
    "\n",
    "The [statsmodel](https://www.statsmodels.org/stable/) is a swiss army knife for creating and observing statistical models. In a few lines of codes, you can be handed a table of statistical summaries. It can be seen as a merger between the worlds of R and python, and functions primarily with `pandas.DataFrame` objects, similar to R's data frames.\n",
    "\n",
    "Below we'll run an Oridinary Least Squares regression model on the data, see a table printing a summary on multiple statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f576ea1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.formula.api import ols # Oridinary Least Squares regression\n",
    "from statsmodels.stats.api import anova_lm # For ANOVA least squares\n",
    "from pandas import DataFrame\n",
    "\n",
    "xy_df = DataFrame(data=xy, columns=[\"height\", \"weight\"]) # statsmodels lives in the world of data frames\n",
    "ols_fit_results = ols(formula=\"height ~ weight\", data=xy_df).fit()\n",
    "\n",
    "print(ols_fit_results.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lessons",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
