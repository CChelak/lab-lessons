{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aee506d8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Basic Regression\n",
    "\n",
    "For this, we'll do a simple linear regression as described [on this Wikipedia page](https://en.wikipedia.org/wiki/Simple_linear_regression), creating a line of best fit through a series of points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f777df",
   "metadata": {},
   "source": [
    "## Basics - Getting warmed up\n",
    "\n",
    "\n",
    "Let's run some basic python. Using the terminal below open up a python instance. Try typing `py` into it. If it fails, try `python.exe`.\n",
    "\n",
    "Finish the following code by first experimenting in the python console, then inserting the answer here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1156817",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_val = 3 # create an integer value\n",
    "list_of_class_nums = [1770, 1000, 2300] # create a list of the class numbers you are taking\n",
    "list_about_me = ['Clint', 4, 1.7] # create a list with your name, birthday month, and your estimated height (meters)\n",
    "\n",
    "# loop through each entry of my_list and print it\n",
    "for val in list_about_me:\n",
    "    print(val) # replace with print statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3f07a3",
   "metadata": {},
   "source": [
    "A list wasn't the most descriptive container for the \"about me\" info. Let's use a dictionary instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9a2ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_about_me = {\"name\":\"Clint\", \"birth_month\":6, \"estimate_height\":1.7} # use keys \"name\", \"birth_month\", \"estimated_height\"\n",
    "\n",
    "# a dictionary has a useful way to loop through keys and values together\n",
    "for key, value in dict_about_me.items():\n",
    "    print(\"key=\", key, \"val=\", value) # replace with print statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c11915e",
   "metadata": {},
   "source": [
    "## Creating your virtual environment\n",
    "\n",
    "We'll install packages locally to the folder you're doing these exercises. This avoids conflicts with PC priveleges and To do this, let's create a virtual environment. The following commands will be run assuming you are using a Windows machine.\n",
    "\n",
    "Run the following from a terminal:\n",
    "\n",
    "```\n",
    "py -m venv lab\n",
    "```\n",
    "\n",
    "A virtual environment should be created in your directory. Look for a folder named `lab`. See what's inside. Look in the windows explorer, or type the following to see the contents:\n",
    "\n",
    "```\n",
    "tree lab\n",
    "```\n",
    "\n",
    "Inside a directory called `Scripts`, there is an `activate` file. In terminal, type the file: `lab\\Scripts\\activate`. Now, your virtual environment is activated, as indicated with the text `(lab)` at the front of the terminal entry line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a58cae-f37f-4840-b265-41bb1bbb841e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Manually Calculating the Linear Regression\n",
    "\n",
    "To get familiar with some python basics, we will manually perform a linear regression in R. Following sections will take advantage of existing libraries to do the leg-work for you.\n",
    "\n",
    "For this, we'll use `numpy`. Numpy is one of the most common python scientific packages. It includes a multi-dimensional array and some high-\n",
    "performance functions and operations on those arrays. I'd highly recommend viewing [their documentation](https://numpy.org/doc/stable/) to get a more-complete understanding. At the very least, go through [their absolute beginners tutorial](https://numpy.org/doc/stable/user/absolute_beginners.html) if you're starting from scratch.\n",
    "\n",
    "You know how I said everything feels very easy to do at first with python? Parts of `numpy` might be an exception. This package is powerful and computationally fast, but from experience teaching it to others, it will feel rigid and a bit confusing. This is probably because it is coded `C`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c522af01-5fae-43ba-8589-5b7584bcc5e0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Reading the data from file\n",
    "\n",
    "We start by importing it. Because it is such a common library, it is convention to shorten the name of `numpy` to `np` with the `as` keyword, as seen below. We'll load the data from file using `np.genfromtxt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbb97f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # for performing arithmetic on homogeneous arrays\n",
    "\n",
    "# create your first numpy array from list_of_class_nums\n",
    "np_class_nums = np.array(list_of_class_nums) # insert fields here\n",
    "\n",
    "# try something with it: let's sort the array from lowest number class to highest\n",
    "np_class_nums.sort()\n",
    "print(np_class_nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8878fe49",
   "metadata": {},
   "source": [
    "Before using the function, briefly review its documentation (either with the `help()` function or with `pydoc3`).\n",
    "\n",
    "* We'll use the `delimiter=','` option to say that entries are comma-separated.\n",
    "* We'll use the `skip_header=True` option to state that we don't care about the names of the columns.\n",
    "* We'll use the `dtype=np.int64` to state that the incoming variables are integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039884a4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "xy = np.genfromtxt('linear_regression_data.csv', delimiter=',', skip_header=True, dtype=np.int64)\n",
    "print(f\"  x y\\n{xy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ff2b8f-cd62-4afc-92ea-02711d1ddc52",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Finding the mean and deviations from mean\n",
    "\n",
    "We can now take this array and use it going forward to find the mean on each column, using the same `numpy` library. Afterwards, we'll find the deviation of each x and y value from the mean.\n",
    "\n",
    "We have to take the mean of each column of our 2D xy array. To do this, we use the `axis` argument. This is often quite confusing to interpret, even for intermediate users, but we specify as the `axis` which dimension we wish to aggregate or collapse down. `axis=0` means to collapse the 0th-indexed dimension, or row. `axis=1` means to collapse the columns. In other words, for a `mean`, specifying `axis=0` will take all row entries per column and find the mean of them. This is what we want.\n",
    "\n",
    "If you ever feel confused, don't be afraid to experiment. Take a small data sample, and run the mean for `axis=0`, then `axis=1` and see which looks right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806fdee9-e61c-4946-b7bd-ba8462b115f3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "xy_mean = xy.mean(axis=0) # calculate the mean using either np.mean or xy.mean, with axis=0\n",
    "\n",
    "print(f\"Mean of x {xy_mean[0]} and mean of y {xy_mean[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600c5ca2",
   "metadata": {},
   "source": [
    "Basic arithmetic of numpy arrays will be performed element-by-element, given that they are the same dimension. When not the same dimension, `numpy` will try to [broadcast](https://numpy.org/doc/stable/user/basics.broadcasting.html) the dimensions to perform element-wise arithmetic. Our `xy_mean` object has two values. Numpy will say, \"well `xy` has 2 columns... let's broadcast the rows.\" Tell me to write the example on the board of how this will work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2162c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_dev = xy - xy_mean# subtract xy from means\n",
    "\n",
    "# check your work!\n",
    "print(f\"Deviations (or least squares residuals) of x and y from its mean:\\n  xdev ydev\\n{xy_dev}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e5f531",
   "metadata": {},
   "source": [
    "### Slicing\n",
    "\n",
    "We need to take another `numpy` aside before proceeding. We need to talk about [indexing and slicing](https://numpy.org/doc/stable/user/basics.indexing.html).\n",
    "\n",
    "For n-dimensional arrays (in our case 2-dimensional `xy`), we can specify the index of the row and column we want to select with a comma `,` within the square brackets. For example, for a 2D array `arr_2d[50,16]` means \"get row index 50 and column index 16\". Let's try it on our `xy` array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f5503c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xy[2,1] # select the third element of the y column\n",
    "\n",
    "# Hint: remember indexing starts at 0, so x is the 0th column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7c8003",
   "metadata": {},
   "source": [
    "We can select a range of elements with the colon `:` operator in the index. Putting numbers on either side of the colon represent the bounds of the range you want. For example, `3:5` means, \"select index 3 up to but not including 5.\" If you know set notation in math, this would be the range $[3,5)$. A colon by itself means select all. I'll give you an example, then we'll practice with `xy_dev` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabd1198",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_1d = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8])\n",
    "print(arr_1d[4:7]) # I want elements in range [4,7)\n",
    "print(arr_1d[:]) # I want everything. In this case, it is the same as leaving off the index\n",
    "\n",
    "xy_dev[:,0] # select every row in the x column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d727bc1c",
   "metadata": {},
   "source": [
    "### Slope\n",
    "\n",
    "We can now calculate the slope using the formula:\n",
    "\n",
    "$$m = \\frac{\\sum ((x_i - \\bar{x}) * (y_i - \\bar{y}))}{\\sum ((x_i - \\bar{x})^2)}$$\n",
    "\n",
    "or:\n",
    "\n",
    "$$m = \\frac{\\sum ( \\textrm{xdev} * \\textrm{ydev})}{\\sum ( \\textrm{xdev}^2)}$$\n",
    "\n",
    "We can use `numpy.sum` to calculate the sum of each point, and the division operator to divide the two sums `/`. Remember that an exponent operator in python is `**`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4245433e-aaec-42ed-9cfe-59c072651be1",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "regression_slope = np.sum(xy_dev[:,0]*xy_dev[:,1]) / np.sum(xy_dev[:,0]**2)\n",
    "print(f\"slope = {regression_slope}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877f13da",
   "metadata": {},
   "source": [
    "Now that we have the slope and mean, the intercept can be found using the formula\n",
    "\n",
    "$$c = \\bar{y} - m * \\bar{x}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8f63a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_intercept = xy_mean[1] - regression_slope * xy_mean[0] # take y_mean - slope * x_mean\n",
    "print(f\"intercept = {regression_intercept}\")\n",
    "\n",
    "print(f\"In the form y = a + bx, we have:\\n  y = {regression_intercept:.2f} + {regression_slope:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1c7747",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Plotting\n",
    "\n",
    "To do this, we'll use `matplotlib`: see [documentation here](https://matplotlib.org/stable/api/index). Let's install it into our environment with `pip`. Afterwards we'll import its `pyplot` module, giving it the shortcut `plt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea8747b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(xy[:,0], xy[:,1], 'go', label=\"Example data\", markersize=10)\n",
    "\n",
    "y_hat = regression_slope * xy[:,0] + regression_intercept\n",
    "plt.plot(xy[:,0], y_hat, label=\"Fitted line\")\n",
    "plt.legend() # add a legend to the graph.\n",
    "plt.show()\n",
    "\n",
    "# Change the marker size, shape and color. Let's use the website or help menu to find out how to change color"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f36f92e",
   "metadata": {},
   "source": [
    "### Finding Sum of Squared values and R-squared\n",
    "\n",
    "We will calculate the following 3 sum of square values:\n",
    "\n",
    "1. Sum of Squares Total -- SST\n",
    "2. Sum of Squares Regression -- SSR\n",
    "3. Sum of Squares Estimate of Errors -- SSE\n",
    "\n",
    "The equations are as follows\n",
    "\n",
    "$$\n",
    "SST = \\sum (y_i - \\bar{y})^2 \\\\\n",
    "SSR = \\sum (\\hat{y}_i â€“ \\bar{y})^2 \\\\\n",
    "SSE = \\sum (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "Where $y_i$ is each observed y entry, $\\hat{y}_i$ is the predicted y entry and $\\bar{y}$ is the mean.\n",
    "\n",
    "Note the following relationship:\n",
    "$$SST = SSR + SSE$$\n",
    "\n",
    "We can calculate all three with our previously calculated values in `xy_dev` and `xy_mean`, and `np.sum`. Just as we did in the plot above, we can calculate all the predicted values with our initial `x` and store it in a variable called `y_hat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056b1a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sst = np.sum(xy_dev[:,1]**2) # we have the x_dev, let's square it\n",
    "print(f\"Sum of Squares Total: {sst:.4f}\")\n",
    "\n",
    "ssr = np.sum((y_hat - xy_mean[1])**2) # use y_hat and mean within the sum\n",
    "print(f\"Sum of Squares Regression: {ssr:.4f}\")\n",
    "\n",
    "sse = sst - ssr# subtract two sum of square values above\n",
    "print(f\"Sum of Squares Estimate of Errors: {sse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3313078",
   "metadata": {},
   "source": [
    "Finally, we can calculate R-squared with the following formula:\n",
    "$$R^2 = \\frac{SSR}{SST}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9955048",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_squared = ssr /sst # simple division\n",
    "print(f\"r-squared = {r_squared}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0444862-7f30-4fd6-b1d1-0e876c3af8e7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Using numpy Least Squares function\n",
    "\n",
    "If we wanted to solve a least-squares regression more optimally using only `numpy`, we can use linear algebra and a tool available in `numpy` already. The `numpy` library has a linear algebra submodule and in there is a least-square solution: `numpy.linalg.lstsq`, see [the help page](https://numpy.org/doc/stable/reference/generated/numpy.linalg.lstsq.html) for their own example.\n",
    "\n",
    "The result of `np.linalg.lstsq` provides a few useful parameters for the data given, such as the solution itself (slope and intercept) along with average of the squared residuals, matrix rank, etc. Really, we're interested in the first things it returns, the solution (i.e. slope, intercept)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cda710a-653b-4383-9587-f44d4a6a60d5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "A = np.array([xy[:,0], np.ones(xy.shape[0])]).T # T is shortcut for \"transpose()\"\n",
    "lstsq_out = np.linalg.lstsq(A, xy[:,1], rcond=None) # Finding the least-squares solution to Ax = y\n",
    "\n",
    "slope, intercept = lstsq_out[0]\n",
    "print(f\"slope = {slope}, intercept = {intercept}\")\n",
    "print(f\"sum of residuals, squared (or SSE): {lstsq_out[1]}\")\n",
    "print(f\"matrix rank = {lstsq_out[2]}\")\n",
    "print(f\"singular values of input (A): {lstsq_out[3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f2c331-4449-4a83-b804-15abd05cb707",
   "metadata": {},
   "source": [
    "Again, we can plot our results, as we saw before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32938e1-92b2-4496-bdc3-f39c98cf8914",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use the matplotlib pyplot. Plot slope and intercept\n",
    "plt.plot(xy[:,0],xy[:,1], 'o', label=\"example data\", markersize=5) # fill in formatting info\n",
    "y_hat = slope * xy[:,0] + intercept # calculate slope * x + intercept with new info above\n",
    "plt.plot(xy[:,0], y_hat, label='Fit line') # add formatting\n",
    "\n",
    "# add a legend to the graph.\n",
    "plt.legend() # add a legend to the graph.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76da7224",
   "metadata": {},
   "source": [
    "## Using statsmodel to tell us everything in the universe\n",
    "\n",
    "The [statsmodel](https://www.statsmodels.org/stable/) is a swiss army knife for creating and observing statistical models. In a few lines of codes, you can be handed a table of statistical summaries. It can be seen as a merger between the worlds of R and python, and functions primarily with `pandas.DataFrame` objects, similar to R's data frames.\n",
    "\n",
    "Below we'll run an Oridinary Least Squares regression model on the data, see a table printing a summary on multiple statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f576ea1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.formula.api import ols # Oridinary Least Squares regression\n",
    "from statsmodels.stats.api import anova_lm # For ANOVA least squares\n",
    "from pandas import DataFrame\n",
    "\n",
    "xy_df = DataFrame(data=xy, columns=[\"height\", \"weight\"]) # statsmodels lives in the world of data frames\n",
    "ols_fit_results = ols(formula=\"height ~ weight\", data=xy_df).fit()\n",
    "\n",
    "print(ols_fit_results.summary())"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Clint Chelak"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "title": "Lab 1 Solutions: Basic Linear Regression"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
